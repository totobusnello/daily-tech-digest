#!/usr/bin/env python3
"""
THE DAILY BYTE - Processador de Curadoria
Usa Claude para filtrar e curar not√≠cias quent√≠ssimas
"""

import os
import json
import anthropic
from datetime import datetime
from pathlib import Path

# ============================================
# CONFIGURA√á√ÉO
# ============================================

ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')
MODEL = "claude-sonnet-4-20250514"
MAX_TOKENS = 4096

# ============================================
# PROMPTS
# ============================================

CURATOR_SYSTEM = """Voc√™ √© o curador do THE DAILY BYTE, um digest de tech/AI para profissionais brasileiros que traz APENAS not√≠cias quent√≠ssimas, primeira m√£o e impactantes.

Sua miss√£o: ZERO mesmice. Os leitores s√£o profissionais de tech que j√° viram tudo.

‚ö†Ô∏è IDIOMA: TODO o output deve ser em PORTUGU√äS BRASILEIRO:
- Headlines em portugu√™s
- "why_it_matters" em portugu√™s
- TL;DR bullets em portugu√™s
- An√°lise do dia em portugu√™s
- Apenas URLs e nomes pr√≥prios (como @sama, OpenAI) ficam em ingl√™s

REGRAS DE OURO:
1. FRESHNESS - S√≥ √∫ltimas 24h, priorize <12h
2. PRIMEIRA M√ÉO - Post do CEO > Artigo sobre o post
3. IMPACTO - Muda o jogo, n√£o incremental
4. EXCLUSIVO - Se j√° vi em 3 newsletters, n√£o √© breaking

Heat Score m√≠nimo para entrar: 60 pontos
- Freshness (40 pts): <6h=40, 6-12h=30, 12-24h=20, >24h=0
- Fonte (30 pts): Fundador=30, Jornalista=25, Release=20, Agregador=0
- Impacto (30 pts): Lan√ßamento=30, M&A=25, Drama=20, Incremental=5

IMPORTANTE: Todo item DEVE ter URL clic√°vel para fonte original."""


CURATOR_USER_TEMPLATE = """Analise estes {total} itens coletados e selecione no M√ÅXIMO 15 para o digest de hoje.

DADOS COLETADOS:
```json
{items}
```

RETORNE JSON com esta estrutura:
{{
  "date": "YYYY-MM-DD",
  "tldr": ["bullet 1", "bullet 2", "bullet 3"],
  "items": [
    {{
      "headline": "Max 12 palavras",
      "why_it_matters": "2 linhas de contexto",
      "source_url": "URL ORIGINAL",
      "source_name": "@handle ou Publica√ß√£o",
      "source_type": "tweet|article|video|paper",
      "hours_ago": 4,
      "heat_score": 75,
      "category": "breaking|ai_models|big_tech|watch_later"
    }}
  ],
  "daily_analysis": "Par√°grafo conectando os pontos do dia",
  "stats": {{
    "total_analyzed": X,
    "selected": Y,
    "rejected_too_old": Z,
    "rejected_low_impact": W
  }}
}}

LEMBRE-SE:
- NO m√°ximo 15 itens selecionados
- Priorize BREAKING real (n√£o requentado)
- Todo item precisa de source_url v√°lida
- Seja impiedoso na curadoria - menos √© mais
- ‚ö†Ô∏è ESCREVA TUDO EM PORTUGU√äS BRASILEIRO (headlines, why_it_matters, tldr, an√°lise)"""


# ============================================
# PROCESSADOR
# ============================================

def load_raw_data(path: str = "/tmp/digest_raw.json") -> dict:
    """Carrega dados brutos do coletor"""
    with open(path, 'r') as f:
        return json.load(f)


def curate_with_claude(raw_data: dict) -> dict:
    """Usa Claude para curar as not√≠cias"""

    if not ANTHROPIC_API_KEY:
        raise ValueError("ANTHROPIC_API_KEY not set")

    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

    # Prepare items (limit to recent and trim content)
    items = raw_data.get('items', [])

    # Pre-filter: only <24h
    items = [i for i in items if i.get('hours_ago', 100) <= 24]

    # Trim content for context
    for item in items:
        if len(item.get('content', '')) > 500:
            item['content'] = item['content'][:500] + '...'

    prompt = CURATOR_USER_TEMPLATE.format(
        total=len(items),
        items=json.dumps(items[:100], ensure_ascii=False, indent=2)  # Max 100 for context
    )

    print(f"ü§ñ Enviando {len(items)} itens para Claude curar...")

    response = client.messages.create(
        model=MODEL,
        max_tokens=MAX_TOKENS,
        system=CURATOR_SYSTEM,
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse response
    response_text = response.content[0].text

    # Extract JSON from response
    try:
        # Try to find JSON in response
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0]
        else:
            json_str = response_text

        curated = json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è Erro parsing JSON: {e}")
        print(f"Response: {response_text[:500]}")
        curated = {"error": str(e), "raw_response": response_text}

    return curated


def save_curated(data: dict, path: str = "/tmp/digest_curated.json"):
    """Salva dados curados"""
    with open(path, 'w') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"üíæ Curadoria salva em {path}")


# ============================================
# MAIN
# ============================================

def process():
    """Pipeline completo de processamento"""
    print("üî• THE DAILY BYTE - Iniciando curadoria...")

    # Load raw data
    raw_data = load_raw_data()
    print(f"üì• Carregados {raw_data['total_items']} itens brutos")

    # Curate with Claude
    curated = curate_with_claude(raw_data)

    # Add metadata
    curated['processed_at'] = datetime.utcnow().isoformat()
    curated['raw_total'] = raw_data['total_items']

    # Save
    save_curated(curated)

    # Summary
    if 'items' in curated:
        print(f"\n‚úÖ Curadoria completa!")
        print(f"   üìä Analisados: {curated.get('stats', {}).get('total_analyzed', '?')}")
        print(f"   ‚ú® Selecionados: {len(curated['items'])}")

        print(f"\nüìå TL;DR:")
        for bullet in curated.get('tldr', []):
            print(f"   ‚Üí {bullet}")

        print(f"\nüî• BREAKING:")
        for item in curated['items'][:5]:
            print(f"   ‚Ä¢ {item.get('headline', '?')}")
            print(f"     Heat: {item.get('heat_score', '?')} | {item.get('source_name', '?')}")

    return curated


if __name__ == "__main__":
    process()
