#!/usr/bin/env python3
"""
THE DAILY BYTE - Processador de Curadoria
Usa Claude para filtrar e curar not√≠cias quent√≠ssimas
"""

import os
import json
import time
import anthropic
from datetime import datetime
from pathlib import Path

# ============================================
# CONFIGURA√á√ÉO
# ============================================

ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')
MODEL = "claude-sonnet-4-20250514"
MAX_TOKENS = 4096

# ============================================
# PROMPTS
# ============================================

CURATOR_SYSTEM = """Voc√™ √© o curador do THE DAILY BYTE, um digest de tech/AI para profissionais brasileiros (CEOs, CFOs, CMOs, CPOs) que traz not√≠cias quent√≠ssimas, primeira m√£o e impactantes.

Sua miss√£o: ZERO mesmice. Os leitores s√£o C-levels de tech que j√° viram tudo.

‚ö†Ô∏è IDIOMA: TODO o output deve ser em PORTUGU√äS BRASILEIRO:
- Headlines em portugu√™s
- "why_it_matters" em portugu√™s
- Se√ß√£o "mundo real" em portugu√™s
- An√°lise do dia em portugu√™s
- Apenas URLs e nomes pr√≥prios (como @sama, OpenAI) ficam em ingl√™s

REGRAS DE OURO:
1. FRESHNESS - S√≥ √∫ltimas 24h, priorize <12h (newsletters: janela de 36h)
2. PRIMEIRA M√ÉO - Post do CEO > Artigo sobre o post
3. IMPACTO PR√ÅTICO - Priorize not√≠cias que afetam o cotidiano de quem trabalha com tech: lan√ßamentos de produtos, mudan√ßas em plataformas, M&A, regula√ß√µes. Papers acad√™micos s√≥ entram se tiverem aplica√ß√£o pr√°tica imediata.
4. EXCLUSIVO - Se j√° vi em 3 newsletters, n√£o √© breaking
5. AN√ÅLISE OBRIGAT√ìRIA - Cada item DEVE ter "why_it_matters" com 2-3 frases de an√°lise contextual. N√£o √© resumo ‚Äî √© o "por que um C-level deveria se importar". Este campo √© ESSENCIAL para o valor do digest.

EQUIL√çBRIO DE CATEGORIAS (obrigat√≥rio):
- "breaking": 3-5 itens (not√≠cias bomb√°sticas do dia)
- "big_tech": 2-4 itens (movimentos de grandes empresas, lan√ßamentos, M&A)
- "ai_models": 2-3 itens (novidades em IA com impacto real)
- "saas_enterprise": 2-3 itens (SaaS, valuations, CapEx, enterprise tech) ‚Äî NOVO
- "tool_of_day": 1 item (UMA ferramenta AI/tech pr√°tica que o leitor pode usar HOJE ‚Äî app, plugin, API, framework. Priorize ferramentas pouco conhecidas mas poderosas.)
- "watch_later": 1-2 itens (v√≠deos ou conte√∫do longo)
Se n√£o houver itens suficientes para uma categoria, tudo bem omitir. Mas NUNCA concentre tudo em uma s√≥ categoria.

SE√á√ÉO MUNDO REAL (obrigat√≥rio):
- Selecione 4-5 not√≠cias do mundo real a partir dos itens com source_type "world" ou "newsletter" com category_hint "world"
- INCLUA not√≠cias do Brasil quando relevantes (economia, mercado, pol√≠tica brasileira)
- Foque em: movimenta√ß√µes de governos, decis√µes pol√≠ticas globais, grandes empresas da economia real (energia, ind√∫stria, infraestrutura, sa√∫de), geopol√≠tica, trade wars, regula√ß√µes
- O objetivo √© tirar o leitor da bolha tech e mostrar o que est√° acontecendo no mundo E no Brasil
- Cada item deve ter: headline curto (max 10 palavras), contexto breve (1 frase), e a URL original
- Priorize impacto global e relev√¢ncia para profissionais brasileiros

REGRAS PARA ITENS DE NEWSLETTER (source_type "newsletter"):
- Newsletters s√£o fontes CURADAS ‚Äî trat√°-las como Tier 2 de confiabilidade
- Quando o mesmo fato aparece em RSS E newsletter, PREFIRA a vers√£o da newsletter se trouxer an√°lise ou contexto adicional
- Se a newsletter apenas REPETE o que o RSS j√° trouxe sem adicionar valor, DESCARTE a duplicata
- Newsletters em portugu√™s podem fornecer o √¢ngulo brasileiro que falta nas fontes internacionais
- Fontes: AiDrop (AI), Evolving AI (AI/modelos), Update Di√°rio (Brasil/geral), TechDrop (SaaS/enterprise), AlphaSignal (research‚Üíproduto)

Heat Score m√≠nimo para entrar: 60 pontos
- Freshness (40 pts): <6h=40, 6-12h=30, 12-24h=20, >24h=0
- Fonte (30 pts): Fundador=30, Jornalista=25, Release=20, Newsletter curada=15, Agregador=0
- Impacto (30 pts): Lan√ßamento=30, M&A=25, Drama=20, Incremental=5
- Newsletter Bonus: Insight exclusivo=+10, Cross-valida√ß√£o=+5

‚ö†Ô∏è REGRA CR√çTICA sobre source_url:
- Todo item DEVE ter o campo "source_url" preenchido com a URL ORIGINAL do artigo/post
- COPIE a URL exatamente como veio nos dados de entrada (campo "url")
- NUNCA deixe source_url vazio, nulo ou inventado
- Se n√£o tiver URL, N√ÉO inclua o item"""


CURATOR_USER_TEMPLATE = """Analise estes {total} itens coletados e selecione no M√ÅXIMO 20 para o digest de hoje.

DADOS COLETADOS:
```json
{items}
```

RETORNE JSON com esta estrutura:
{{
  "date": "YYYY-MM-DD",
  "world": [
    {{
      "headline": "Max 10 palavras",
      "context": "1 frase de contexto",
      "source_url": "URL ORIGINAL",
      "source_name": "Reuters|Forbes|BBC"
    }}
  ],
  "items": [
    {{
      "headline": "Max 12 palavras",
      "why_it_matters": "OBRIGAT√ìRIO: 2-3 frases de an√°lise explicando POR QUE esta not√≠cia importa para o leitor. N√£o √© resumo ‚Äî √© contexto estrat√©gico e impacto pr√°tico.",
      "source_url": "URL ORIGINAL",
      "source_name": "@handle ou Publica√ß√£o",
      "source_type": "tweet|article|video|paper",
      "hours_ago": 4,
      "heat_score": 75,
      "category": "breaking|ai_models|big_tech|saas_enterprise|tool_of_day|watch_later"
    }}
  ],
  "daily_analysis": [
    "**Tema curto** ‚Äî Insight conectando pontos do dia em 1-2 frases",
    "**Outro tema** ‚Äî Outro insight relevante",
    "**Tend√™ncia** ‚Äî O que isso sinaliza para o futuro pr√≥ximo"
  ],
  "stats": {{
    "total_analyzed": X,
    "selected": Y,
    "rejected_too_old": Z,
    "rejected_low_impact": W
  }}
}}

LEMBRE-SE:
- NO m√°ximo 20 itens selecionados
- Priorize BREAKING real (n√£o requentado)
- Todo item precisa de source_url v√°lida
- Seja impiedoso na curadoria - menos √© mais
- Inclua itens de NEWSLETTER quando trouxerem an√°lise ou √¢ngulo √∫nico
- A categoria "saas_enterprise" cobre: SaaS, CapEx, valuations, enterprise tech
- Na se√ß√£o "world", inclua pelo menos 1 not√≠cia relevante do Brasil quando dispon√≠vel
- ‚ö†Ô∏è ESCREVA TUDO EM PORTUGU√äS BRASILEIRO (headlines, why_it_matters, mundo real, an√°lise)

‚ö†Ô∏è REGRA CR√çTICA sobre why_it_matters:
- CADA item DEVE ter um "why_it_matters" com 2-3 frases SUBSTANCIAIS
- N√ÉO √© um resumo da not√≠cia ‚Äî √© uma AN√ÅLISE do impacto e contexto
- Responda: "Por que um CEO/CFO/CMO/CPO deveria se importar com isso?"
- Conecte com tend√™ncias maiores, impacto no mercado, ou a√ß√£o pr√°tica
- NUNCA deixe why_it_matters vazio ou com apenas 1 frase curta"""


# ============================================
# PROCESSADOR
# ============================================

def load_raw_data(path: str = "/tmp/digest_raw.json") -> dict:
    """Carrega dados brutos do coletor"""
    with open(path, 'r') as f:
        return json.load(f)


def curate_with_claude(raw_data: dict) -> dict:
    """Usa Claude para curar as not√≠cias"""

    if not ANTHROPIC_API_KEY:
        raise ValueError("ANTHROPIC_API_KEY not set")

    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

    # Prepare items (limit to recent and trim content)
    items = raw_data.get('items', [])

    # Pre-filter: <24h for regular sources, <36h for newsletters
    items = [i for i in items if (
        i.get('hours_ago', 100) <= 36 if i.get('source_type') == 'newsletter'
        else i.get('hours_ago', 100) <= 24
    )]

    # Trim content for context
    for item in items:
        if len(item.get('content', '')) > 500:
            item['content'] = item['content'][:500] + '...'

    prompt = CURATOR_USER_TEMPLATE.format(
        total=len(items),
        items=json.dumps(items[:40], ensure_ascii=False, indent=2)  # Max 40 items (increased for newsletters)
    )

    print(f"ü§ñ Enviando {len(items)} itens para Claude curar...")

    for attempt in range(3):
        try:
            response = client.messages.create(
                model=MODEL,
                max_tokens=MAX_TOKENS,
                system=CURATOR_SYSTEM,
                messages=[{"role": "user", "content": prompt}]
            )
            break
        except anthropic.RateLimitError:
            wait = 60 * (attempt + 1)
            print(f"‚è≥ Rate limit atingido, aguardando {wait}s (tentativa {attempt + 1}/3)...")
            time.sleep(wait)
    else:
        raise RuntimeError("‚ùå Rate limit persistente ap√≥s 3 tentativas")

    # Parse response
    response_text = response.content[0].text

    # Extract JSON from response
    try:
        # Try to find JSON in response
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0]
        else:
            json_str = response_text

        curated = json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è Erro parsing JSON: {e}")
        print(f"Response: {response_text[:500]}")
        curated = {"error": str(e), "raw_response": response_text}

    return curated


def save_curated(data: dict, path: str = "/tmp/digest_curated.json"):
    """Salva dados curados"""
    with open(path, 'w') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"üíæ Curadoria salva em {path}")


# ============================================
# MAIN
# ============================================

def process():
    """Pipeline completo de processamento"""
    print("üî• THE DAILY BYTE - Iniciando curadoria...")

    # Check for override file (resend)
    override_path = Path(__file__).parent / "resend_curated.json"
    if override_path.exists():
        print("üì¶ Usando curadoria override (resend)...")
        with open(override_path, 'r') as f:
            curated = json.load(f)
        save_curated(curated)
        print(f"‚úÖ Override aplicado com {len(curated.get('items', []))} itens")
        return curated

    # Load raw data
    raw_data = load_raw_data()
    print(f"üì• Carregados {raw_data['total_items']} itens brutos")

    # Curate with Claude
    curated = curate_with_claude(raw_data)

    # Add metadata
    curated['processed_at'] = datetime.utcnow().isoformat()
    curated['raw_total'] = raw_data['total_items']

    # Save
    save_curated(curated)

    # Summary
    if 'items' in curated:
        print(f"\n‚úÖ Curadoria completa!")
        print(f"   üìä Analisados: {curated.get('stats', {}).get('total_analyzed', '?')}")
        print(f"   ‚ú® Selecionados: {len(curated['items'])}")

        print(f"\nüåç MUNDO REAL:")
        for item in curated.get('world', []):
            print(f"   ‚Üí {item.get('headline', '?')}")

        print(f"\nüî• BREAKING:")
        for item in curated['items'][:5]:
            print(f"   ‚Ä¢ {item.get('headline', '?')}")
            print(f"     Heat: {item.get('heat_score', '?')} | {item.get('source_name', '?')}")

    return curated


if __name__ == "__main__":
    process()
